{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOts76p6XoN0XAJd7QK60ri",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/celiolarcher/knowledge_repository/blob/main/finetuning_llm/Training_Model_with_QLORA_and_DeepSpeed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetunning with QLORA"
      ],
      "metadata": {
        "id": "MZM-nbr-mmhV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Configuration"
      ],
      "metadata": {
        "id": "7SxOQ62xmm59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers bitsandbytes peft accelerate datasets einops deepspeed"
      ],
      "metadata": {
        "id": "e60nztoumwSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "nVZS4OCwm8xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Model"
      ],
      "metadata": {
        "id": "Lq16G57hn9SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = 'tiiuae/falcon-40b'\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map = \"auto\",\n",
        "                                             quantization_config=nf4_config,\n",
        "                                             trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "GQUdRdsLoAi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "yhZyVaEZodr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)"
      ],
      "metadata": {
        "id": "3ZF9e3qnoR0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Find all linear layers to apply LORA, except those excluded by quantization and lm_head\n",
        "def find_all_linear_names(model):\n",
        "    import bitsandbytes as bnb\n",
        "\n",
        "    cls = bnb.nn.Linear4bit ## Fix as 4bits quantization\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "\n",
        "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "    return list(lora_module_names)\n",
        "\n",
        "modules = find_all_linear_names(model)"
      ],
      "metadata": {
        "id": "cvytJKEoq2kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modules"
      ],
      "metadata": {
        "id": "zyjWvB97r4Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    target_modules = modules,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "Eb3AeKKOpTih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "KsEx4fzQqOGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "1ert1s7uqPk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "8oN83a4xsh6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CUTOFF_LEN = 512"
      ],
      "metadata": {
        "id": "NJQcYuh66_FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/22-hours/cabrita/main/data/cabrita-dataset-52k.json"
      ],
      "metadata": {
        "id": "7n5_V5ncRSjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"cabrita-dataset-52k.json\")"
      ],
      "metadata": {
        "id": "QCK7gTJix52Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(instruction, input, output=None):\n",
        "  if input:\n",
        "    prompt = f\"\"\"Abaixo está uma instrução que descreve uma tarefa, juntamente com uma entrada que fornece mais contexto. Escreva uma resposta que complete adequadamente o pedido.\n",
        "### Instrução:\n",
        "{instruction}\n",
        "### Entrada:\n",
        "{input}\n",
        "### Resposta:\n",
        "\"\"\"\n",
        "  else:\n",
        "    prompt = f\"\"\"Abaixo está uma instrução que descreve uma tarefa. Escreva uma resposta que complete adequadamente o pedido.\n",
        "### Instrução:\n",
        "{instruction}\n",
        "### Resposta:\n",
        "\"\"\"\n",
        "  if output:\n",
        "    prompt = f\"{prompt}{output}\"\n",
        "\n",
        "  return prompt\n",
        "\n",
        "\n",
        "def tokenize(prompt, add_eos_token=True):\n",
        "    result = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=CUTOFF_LEN,\n",
        "        padding=False,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    if (\n",
        "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
        "        and len(result[\"input_ids\"]) < CUTOFF_LEN\n",
        "        and add_eos_token\n",
        "    ):\n",
        "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
        "        result[\"attention_mask\"].append(1)\n",
        "\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def generate_and_tokenize_prompt(data_point):\n",
        "\n",
        "    full_prompt = generate_prompt(\n",
        "        data_point[\"instruction\"],\n",
        "        data_point[\"input\"],\n",
        "        data_point[\"output\"],\n",
        "    )\n",
        "    tokenized_full_prompt = tokenize(full_prompt)\n",
        "\n",
        "    user_prompt = generate_prompt(\n",
        "        data_point[\"instruction\"], data_point[\"input\"]\n",
        "    )\n",
        "    tokenized_user_prompt = tokenize(user_prompt, add_eos_token=False)\n",
        "    user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
        "\n",
        "    tokenized_full_prompt[\"labels\"] = [\n",
        "        -100\n",
        "    ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
        "        user_prompt_len:\n",
        "    ]\n",
        "    return tokenized_full_prompt\n"
      ],
      "metadata": {
        "id": "ZdWVObS-3CWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = dataset.map(\n",
        "    generate_and_tokenize_prompt,\n",
        "    batched=False,\n",
        "    num_proc=4,\n",
        "    remove_columns=['instruction', 'input', 'output'],\n",
        "    load_from_cache_file=True,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "4eeQ_O8j3kxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "os.environ[\"MASTER_PORT\"] = \"9994\"  # modify if RuntimeError: Address already in use\n",
        "os.environ[\"RANK\"] = \"0\"\n",
        "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
        "os.environ[\"WORLD_SIZE\"] = \"1\""
      ],
      "metadata": {
        "id": "AUwXUTUpR5kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deepspeed_config = {\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"initial_scale_power\": 16,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": \"auto\",\n",
        "            \"betas\": \"auto\",\n",
        "            \"eps\": \"auto\",\n",
        "            \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"WarmupLR\",\n",
        "        \"params\": {\n",
        "            \"warmup_min_lr\": \"auto\",\n",
        "            \"warmup_max_lr\": \"auto\",\n",
        "            \"warmup_num_steps\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": True\n",
        "        },\n",
        "        \"offload_param\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": True\n",
        "        },\n",
        "        \"overlap_comm\": True,\n",
        "        \"contiguous_gradients\": True,\n",
        "        \"sub_group_size\": 1e9,\n",
        "        \"reduce_bucket_size\": \"auto\",\n",
        "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
        "        \"stage3_param_persistence_threshold\": \"auto\",\n",
        "        \"stage3_max_live_parameters\": 1e9,\n",
        "        \"stage3_max_reuse_distance\": 1e9,\n",
        "        \"stage3_gather_16bit_weights_on_model_save\": True\n",
        "    },\n",
        "\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"gradient_clipping\": \"auto\",\n",
        "    \"steps_per_print\": 2000,\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": False\n",
        "}"
      ],
      "metadata": {
        "id": "keRYjE5AjQLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, GenerationConfig, DataCollatorForSeq2Seq, set_seed\n",
        "from accelerate.utils import DummyOptim, DummyScheduler\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "EPOCHS = 1\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "MICRO_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-4\n",
        "WARMUP_STEPS = 100\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer, model),\n",
        "    args=Seq2SeqTrainingArguments(\n",
        "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        logging_steps=200,\n",
        "        output_dir=\"qlora-cabrita\",\n",
        "        save_total_limit=3,\n",
        "        gradient_checkpointing=True,\n",
        "        deepspeed=deepspeed_config,\n",
        "        generation_config = GenerationConfig(temperature=0)\n",
        "    )\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train(resume_from_checkpoint=False)"
      ],
      "metadata": {
        "id": "lGBfhPMVqdSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-obwelSOrwM5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}