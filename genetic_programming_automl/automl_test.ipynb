{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits=load_digits()\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/Data/Github/random_code/env/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/media/Data/Github/random_code/env/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/media/Data/Github/random_code/env/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/media/Data/Github/random_code/env/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/media/Data/Github/random_code/env/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/media/Data/Github/random_code/env/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/media/Data/Github/random_code/env/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/media/Data/Github/random_code/env/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/media/Data/Github/random_code/env/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best pipeline: KNeighborsClassifier(input_matrix, n_neighbors=2, p=2, weights=distance)\n",
      "0.9911111111111112\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from tpot import config\n",
    "\n",
    "tpot_opt = TPOTClassifier(generations=5, population_size=50, n_jobs = 8, cv=3, config_dict = config.classifier_config_dict_light, verbosity=1, random_state=42)\n",
    "tpot_opt.fit(X_train, y_train)\n",
    "\n",
    "print(tpot_opt.score(X_test, y_test))\n",
    "tpot_opt.export('tpot_digits_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sklearn.naive_bayes.GaussianNB': {},\n",
       " 'sklearn.naive_bayes.BernoulliNB': {'alpha': [0.001,\n",
       "   0.01,\n",
       "   0.1,\n",
       "   1.0,\n",
       "   10.0,\n",
       "   100.0],\n",
       "  'fit_prior': [True, False]},\n",
       " 'sklearn.naive_bayes.MultinomialNB': {'alpha': [0.001,\n",
       "   0.01,\n",
       "   0.1,\n",
       "   1.0,\n",
       "   10.0,\n",
       "   100.0],\n",
       "  'fit_prior': [True, False]},\n",
       " 'sklearn.tree.DecisionTreeClassifier': {'criterion': ['gini', 'entropy'],\n",
       "  'max_depth': range(1, 11),\n",
       "  'min_samples_split': range(2, 21),\n",
       "  'min_samples_leaf': range(1, 21)},\n",
       " 'sklearn.neighbors.KNeighborsClassifier': {'n_neighbors': range(1, 101),\n",
       "  'weights': ['uniform', 'distance'],\n",
       "  'p': [1, 2]},\n",
       " 'sklearn.linear_model.LogisticRegression': {'penalty': ['l1', 'l2'],\n",
       "  'C': [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 15.0, 20.0, 25.0],\n",
       "  'dual': [True, False]},\n",
       " 'sklearn.preprocessing.Binarizer': {'threshold': array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ,\n",
       "         0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])},\n",
       " 'sklearn.cluster.FeatureAgglomeration': {'linkage': ['ward',\n",
       "   'complete',\n",
       "   'average'],\n",
       "  'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine']},\n",
       " 'sklearn.preprocessing.MaxAbsScaler': {},\n",
       " 'sklearn.preprocessing.MinMaxScaler': {},\n",
       " 'sklearn.preprocessing.Normalizer': {'norm': ['l1', 'l2', 'max']},\n",
       " 'sklearn.decomposition.PCA': {'svd_solver': ['randomized'],\n",
       "  'iterated_power': range(1, 11)},\n",
       " 'sklearn.kernel_approximation.RBFSampler': {'gamma': array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ,\n",
       "         0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])},\n",
       " 'sklearn.preprocessing.RobustScaler': {},\n",
       " 'sklearn.preprocessing.StandardScaler': {},\n",
       " 'tpot.builtins.ZeroCount': {},\n",
       " 'sklearn.feature_selection.SelectFwe': {'alpha': array([0.   , 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008,\n",
       "         0.009, 0.01 , 0.011, 0.012, 0.013, 0.014, 0.015, 0.016, 0.017,\n",
       "         0.018, 0.019, 0.02 , 0.021, 0.022, 0.023, 0.024, 0.025, 0.026,\n",
       "         0.027, 0.028, 0.029, 0.03 , 0.031, 0.032, 0.033, 0.034, 0.035,\n",
       "         0.036, 0.037, 0.038, 0.039, 0.04 , 0.041, 0.042, 0.043, 0.044,\n",
       "         0.045, 0.046, 0.047, 0.048, 0.049]),\n",
       "  'score_func': {'sklearn.feature_selection.f_classif': None}},\n",
       " 'sklearn.feature_selection.SelectPercentile': {'percentile': range(1, 100),\n",
       "  'score_func': {'sklearn.feature_selection.f_classif': None}},\n",
       " 'sklearn.feature_selection.VarianceThreshold': {'threshold': [0.0001,\n",
       "   0.0005,\n",
       "   0.001,\n",
       "   0.005,\n",
       "   0.01,\n",
       "   0.05,\n",
       "   0.1,\n",
       "   0.2]}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.classifier_config_dict_light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_config = \\\n",
    "{'sklearn.naive_bayes.GaussianNB': {},\n",
    " 'sklearn.naive_bayes.BernoulliNB': \n",
    "    {'alpha': [0.001,\n",
    "    0.01,\n",
    "    0.1,\n",
    "    1.0,\n",
    "    10.0,\n",
    "    100.0],\n",
    "    'fit_prior': [True, False]},\n",
    " 'sklearn.naive_bayes.MultinomialNB': \n",
    "    {'alpha': [0.001,\n",
    "    0.01,\n",
    "    0.1,\n",
    "    1.0,\n",
    "    10.0,\n",
    "    100.0],\n",
    "    'fit_prior': [True, False]},\n",
    "\n",
    "    'sklearn.preprocessing.RobustScaler': {\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.StandardScaler': {\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9022222222222223\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, n_jobs = 8, cv=2, config_dict=sample_config, random_state=42)\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_digits_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-CVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATASET\n",
      "LOADED DATASET\n",
      "GENERATION 0\n",
      "[11:45:17] WARNING: /tmp/pip-install-lijjxs85/xgboost/build/temp.linux-x86_64-3.7/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "GENERATION 1 (1 secs)\n",
      "[11:45:18] WARNING: /tmp/pip-install-lijjxs85/xgboost/build/temp.linux-x86_64-3.7/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "GENERATION 2 (2 secs)\n",
      "GENERATION 3 (4 secs)\n",
      "GENERATION 4 (6 secs)\n",
      "GENERATION 5 (8 secs)\n",
      "END EVOLUTION (11 secs)\n",
      "BEGIN CROSS VALIDATION\n",
      "END PROCESS (25 secs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from AUTOCVE.AUTOCVE import AUTOCVEClassifier\n",
    "\n",
    "autocve=AUTOCVEClassifier(generations=5, population_size_components=10, n_jobs=8, verbose=0)\n",
    "\n",
    "autocve.optimize(X_train, y_train, subsample_data=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pipe_0', Pipeline(steps=[('zerocount', ZeroCount()), ('minmaxscaler', MinMaxScaler()),\n",
      "                ('kneighborsclassifier',\n",
      "                 KNeighborsClassifier(n_jobs=1, n_neighbors=1,\n",
      "                                      weights='distance'))])), ('Pipe_1', Pipeline(steps=[('zerocount', ZeroCount()),\n",
      "                ('rfe',\n",
      "                 RFE(estimator=ExtraTreesClassifier(max_features=0.525,\n",
      "                                                    n_jobs=1, random_state=42),\n",
      "                     step=0.145)),\n",
      "                ('multinomialnb', MultinomialNB(alpha=0.001, fit_prior=False))])), ('Pipe_2', Pipeline(steps=[('zerocount', ZeroCount()), ('minmaxscaler', MinMaxScaler()),\n",
      "                ('kneighborsclassifier',\n",
      "                 KNeighborsClassifier(n_jobs=1, n_neighbors=1,\n",
      "                                      weights='distance'))])), ('Pipe_3', Pipeline(steps=[('onehotencoder',\n",
      "                 OneHotEncoder(minimum_fraction=0.15, sparse=False)),\n",
      "                ('minmaxscaler', MinMaxScaler()),\n",
      "                ('linearsvc',\n",
      "                 LinearSVC(C=5.0, loss='hinge', random_state=42, tol=1e-05))])), ('Pipe_4', Pipeline(steps=[('zerocount', ZeroCount()), ('minmaxscaler', MinMaxScaler()),\n",
      "                ('kneighborsclassifier',\n",
      "                 KNeighborsClassifier(n_jobs=1, n_neighbors=16,\n",
      "                                      weights='distance'))]))]\n",
      "Ensemble size: 5\n",
      "Train Score: 1.00\n",
      "Test Score: 0.98\n"
     ]
    }
   ],
   "source": [
    "best_voting_ensemble=autocve.get_best_voting_ensemble()\n",
    "print(best_voting_ensemble.estimators)\n",
    "\n",
    "print(\"Ensemble size: \"+str(len(best_voting_ensemble.estimators)))\n",
    "\n",
    "\n",
    "best_voting_ensemble.fit(X_train, y_train)\n",
    "print(\"Train Score: {:.2f}\".format(best_voting_ensemble.score(X_train, y_train)))\n",
    "print(\"Test Score: {:.2f}\".format(best_voting_ensemble.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATASET\n",
      "LOADED DATASET\n",
      "GENERATION 0\n",
      "GENERATION 1 (5 secs)\n",
      "GENERATION 2 (11 secs)\n",
      "GENERATION 3 (18 secs)\n",
      "GENERATION 4 (28 secs)\n",
      "GENERATION 5 (42 secs)\n",
      "END EVOLUTION (55 secs)\n",
      "BEGIN CROSS VALIDATION\n",
      "END PROCESS (135 secs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from AUTOCVE.AUTOCVE import AUTOCVEClassifier\n",
    "\n",
    "autocve=AUTOCVEClassifier(generations=5, population_size_components=10, n_jobs=8, grammar='./grammarSample', verbose=0)\n",
    "\n",
    "autocve.optimize(X_train, y_train, subsample_data=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pipe_0', MultinomialNB(alpha=0.01, fit_prior=False)), ('Pipe_1', Pipeline(steps=[('onehotencoder-1',\n",
      "                 OneHotEncoder(minimum_fraction=0.15, sparse=False)),\n",
      "                ('zerocount', ZeroCount()),\n",
      "                ('onehotencoder-2',\n",
      "                 OneHotEncoder(minimum_fraction=0.2, sparse=False)),\n",
      "                ('extratreesclassifier',\n",
      "                 ExtraTreesClassifier(criterion='entropy', max_features=0.24,\n",
      "                                      min_samples_leaf=6, min_samples_split=15,\n",
      "                                      n_jobs=1, random_state=42))])), ('Pipe_2', Pipeline(steps=[('onehotencoder',\n",
      "                 OneHotEncoder(minimum_fraction=0.05, sparse=False)),\n",
      "                ('zerocount-1', ZeroCount()), ('zerocount-2', ZeroCount()),\n",
      "                ('zerocount-3', ZeroCount()),\n",
      "                ('rfe',\n",
      "                 RFE(estimator=ExtraTreesClassifier(criterion='entropy',\n",
      "                                                    max_features=0.953,\n",
      "                                                    n_jobs=1,\n",
      "                                                    random_state=42))),\n",
      "                ('minmaxscaler', MinMaxScaler()),\n",
      "                ('kneighborsclassifier',\n",
      "                 KNeighborsClassifier(n_jobs=1, n_neighbors=1, p=1,\n",
      "                                      weights='distance'))])), ('Pipe_3', Pipeline(steps=[('onehotencoder-1',\n",
      "                 OneHotEncoder(minimum_fraction=0.1, sparse=False)),\n",
      "                ('zerocount-1', ZeroCount()),\n",
      "                ('onehotencoder-2',\n",
      "                 OneHotEncoder(minimum_fraction=0.1, sparse=False)),\n",
      "                ('onehotencoder-3',\n",
      "                 OneHotEncoder(minimum_fraction=0.05, sparse=False)),\n",
      "                ('onehotencoder-4',\n",
      "                 OneHotEncoder(minimum_fraction=0.05, sparse=False)),\n",
      "                ('zerocount-2', ZeroCount()),\n",
      "                ('rfe',\n",
      "                 RFE(estimator=ExtraTreesClassifier(criterion='entropy',\n",
      "                                                    max_features=0.05, n_jobs=1,\n",
      "                                                    random_state=42))),\n",
      "                ('minmaxscaler', MinMaxScaler()),\n",
      "                ('kneighborsclassifier',\n",
      "                 KNeighborsClassifier(n_jobs=1, n_neighbors=1, p=1,\n",
      "                                      weights='distance'))]))]\n",
      "Ensemble size: 4\n",
      "Train Score: 1.00\n",
      "Test Score: 0.98\n"
     ]
    }
   ],
   "source": [
    "best_voting_ensemble=autocve.get_best_voting_ensemble()\n",
    "print(best_voting_ensemble.estimators)\n",
    "\n",
    "print(\"Ensemble size: \"+str(len(best_voting_ensemble.estimators)))\n",
    "\n",
    "\n",
    "best_voting_ensemble.fit(X_train, y_train)\n",
    "print(\"Train Score: {:.2f}\".format(best_voting_ensemble.score(X_train, y_train)))\n",
    "print(\"Test Score: {:.2f}\".format(best_voting_ensemble.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML-Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalation\n",
    "\n",
    "AutoML-Zero requires a few more steps to be running. \n",
    "\n",
    "- Install bazel (https://github.com/bazelbuild/bazel/releases)\n",
    "- export PATH=\"$PATH:$HOME/bin\"\n",
    "\n",
    "After the bazel instalation, the repository requires some adaptations on the WORKSPACE file, replacing the word master to main on this parameter:\n",
    "\n",
    "```\n",
    "http_archive(\n",
    "    name = \"rules_cc\",\n",
    "    strip_prefix = \"rules_cc-main\",\n",
    "    urls = [\"https://github.com/bazelbuild/rules_cc/archive/main.zip\"],\n",
    ")\n",
    "```\n",
    "\n",
    "and replacing this parameter\n",
    "\n",
    "```\n",
    "http_archive(\n",
    "    name = \"rules_proto\",\n",
    "    strip_prefix = \"rules_proto-master\",\n",
    "    urls = [\n",
    "        \"https://github.com/bazelbuild/rules_proto/archive/master.zip\",\n",
    "    ],\n",
    ")\n",
    "```\n",
    "to \n",
    "\n",
    "```\n",
    "load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\", \"http_file\") # used to pull in grpc\n",
    "load(\"@bazel_tools//tools/build_defs/repo:git.bzl\", \"git_repository\")\n",
    "git_repository(\n",
    "    name = \"rules_proto\",\n",
    "    commit = \"f7a30f6f80006b591fa7c437fe5a951eb10bcbcf\", # Latest commit working\n",
    "    remote = \"https://github.com/bazelbuild/rules_proto.git\",\n",
    ")\n",
    "load(\"@rules_proto//proto:repositories.bzl\", \"rules_proto_dependencies\", \"rules_proto_toolchains\")\n",
    "rules_proto_dependencies()\n",
    "rules_proto_toolchains()\n",
    "\n",
    "load(\"@com_google_protobuf//:protobuf_deps.bzl\", \"protobuf_deps\")\n",
    "\n",
    "protobuf_deps()\n",
    "```\n",
    "\n",
    "After these adaptarions, just run ./run_baseline.sh as in documentation to perform all experiments. (use ./run_demo.sh to test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some interesting code files\n",
    "\n",
    "- algorithm.h/algorithm.cc -> it has the individual codification as an algorithm\n",
    "- instruction.h/instruction.cc -> it has all the operators (commands) used to create an algorithm\n",
    "- run_search_experiment.cc -> it has the evolutionary procedure coded"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b444b43073dd3eecab0454127d865569fd4ec48ed084f873efec0e8f5dbcdf0c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('plot_figures': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
